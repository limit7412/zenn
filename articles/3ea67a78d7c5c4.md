---
title: "モバイル版Goghバックエンドを振り返る"
emoji: "⏳️"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["アーキテクチャ", "aws", "go", "planetscale", "momento"]
published: true
publication_name: ambr_inc
---
## はじめに

:::message
この記事は[ambr, Inc. Advent Calendar 2025](https://adventar.org/calendars/11923)の12/24の記事です。
:::

:::message
この記事は[ambr Tech Blog](https://zenn.dev/p/ambr_inc)のために寄稿されたものです。
:::

こんにちは。
C to C接続の有線VRHMDを買ってからというものデータケーブル沼にはまり込んでしまっているambrサーバーサイドエンジニアの回路（[@qazx7412](https://zenn.dev/qazx7412)）です。

少し早くはありますが、本年は弊社のプロダクト、サービスへの数々のご愛顧ありがとうございました。
また来年も引き続きよろしくお願いいたします。

というわけで今年も終わりに近づいているということで、せっかくなので私がリードエンジニアを担当するモバイル版Goghのバックエンドに関してここまでの振り返りをしていこうかなと思います。

今回は振り返りという名のポエムなので形式ばらずに適当に書き散らかして行こうと思います。

## 開発前史:引き継がれたコードベース

Goghの開発自体は2023年後半、当時まだ運用中であったxambr^[弊社が運用していたVRイベントを開催するプラットフォーム。現在はclose済み]からforkする形で始まりました。
xambrのコードベースやインフラはその少し前に大きく抜本的な変更をしていたのですが、いくつかの基盤となる機能やインフラ周りで引き継がれた設定項目などのそれ以前のコードなども残っており、そのいくつかはGoghにも引き継がれることになりました。
xambr自体はfork後にまたアーキテクチャ戦略を変更し、現在のGoghとは違う方向に進んでいくことになったのですがそれは別の話。

ともかくGoghのバックエンドはxambrやそれ以前^[xambrにはその前身となるプロダクトとしてVRSNS「仮想世界ambr」というものが存在した]のambrのバックエンド開発の積み重ねの上に建つことになりました。

## インフラの技術選定:サーバーレスへの期待

前述の通りこのバックエンドはxambrからのforkなのですが、当然細かい設定値を変えつつそのまま引き継いだものと刷新したもの、また新たに作成したものがあります。
Goghではそれぞれ別の理由でRDBとキャッシュDBに関してxambrから変更しました。
全体の方針としてはできるだけサーバーレス、マネージドサービスに寄せるという方針にしています。
これは運用負荷をできるだけ低く抑えていくことと、もう1つはスケールさせやすい構成にするのが狙いでした。
前者に関しては長期の運用が前提となることを見越して運用に関わるコスト（これは価格的、人的なもの両方）をできるだけ減らしたいということ、後者に関してはそもそもこのGoghというプロダクトに対して前例として参考にできるデータが弊社になく、どれだけの速度、ペースでユーザーが増えていくか見えない中でスケーリングに対して柔軟に対応できることを期待するものでした。

### APIサーバー:マネージドに寄せていく

メインとなるAPIサーバーにはxambrから継続でApp Runnerを採用しました。

@[card](https://aws.amazon.com/jp/apprunner/)

App Runnerにはホットスタンバイでインスタンスとして待機している「プロビジョニングされたコンテナインスタンス」とアクティブな状態として実際に来たリクエストを捌く「アクティブなコンテナインスタンス」がそれぞれ別の概念として独立し、別々に課金されるようになっています。
その上でアクティブなインスタンスがゼロスケールするという仕様があり、xambrはイベント開催時には大きなアクセスがあるがそれ以外ではほとんどアクセスがない特性を持っていたので、長期の運用を見越したときこちらのほうが有利だと判断して当時の選定時に採用してもらった^[この書き方なのは当時xambrは私がリードエンジニアではなかったから]経緯がありました。
ただGoghでの採用理由はそこよりは純粋に（対ECS/Fargate比で）抽象度の高いサービスを使用して運用負荷を下げることが狙いでした。

### RDB:NewSQLとブランチ

RDBには新規でPlanetScaleを採用することにしました。

@[card](https://planetscale.com)

PlanetScaleはいわゆるところのNewSQLに分類されるDBaaSでAurora等のようにマネージドにRDBを使用できるサービスです。
元々は開発を開始した当時、最終的にどれくらいの負荷になるか見えていないところもあり、スケーリングに強い構成にしたかったというのが狙いの1つでした。
特に当時はサーバーレスDBのような謳い文句で売り出しており、PlanetScaleは現在の料金プランとは違うサーバーレス色の強いサービスだったというのもありました。
ただこれに関してはGoghのリリースが近いタイミング（2024年4月）でHobbyプランが廃止されScalerプランへの移行が必要となり、通常のインスタンスの存在を意識するようなサービスに変更されてしまいました。
タイミング的にはサーバーレスDBとしては競合となる[TiDB Serverless](https://pingcap.co.jp/tidb/)も存在した（はず^[流石にタイミングとかの記憶は曖昧…]）ので、そちらを選定する可能性もあったのですが、Goghで想定された要件としてユーザー名検索というのがあり、PlanetScaleではMySQLの全文検索を使用できたのでそのままPlanetScaleを使用する判断となりました。

またもう1つの狙いとしてPlanetScaleのブランチ機能によるマイグレーション作業の自動化がありました。

@[card](https://planetscale.com/docs/vitess/schema-changes/branching)

ブランチ機能はDBに対してGitリポジトリのようにブランチを切ることができる機能です。
ここで切れるブランチは本番環境となるmainブランチからスキーマをコピーした独立した環境となっていて、本番に影響のない形でスキーマ変更を行えます。
そしてこの切ったブランチに入れた変更をPRならぬDeploy Requestを作成し変更差分のレビューと、そこから自動でのマイグレーションを行うことができるようになっています。
例えば「main（prod）→stg→dev→作業ブランチ」というようにブランチを切れば作業ブランチに変更を入れ、そのチームでのレビュー、リリースサイクルに応じてマージをしていくことによって各環境での手動でのマイグレーションを行わず変更を反映していくことができます。

### キャッシュDB:クラスタリングからの解放

キャッシュではRDBと同じくサーバーレスなキャッシュとしてMomento（正確にはMomento Cacheだが他サービスは使用しないのと一般的にもそう呼ばれるので以降Momentoで統一）を採用しました。

@[card](https://www.gomomento.com/jp/)

確か当時開発中のAWS Summitのイベントで紹介をしていただいたのがきっかけだったと記憶しています。
元々xambrではクラスタリングをしないElastiCache for Redisで運用をしていました。
一応アプリケーション上はクラスタリングを想定した実装は存在したのですが、これには本番環境での稼働実績がなく使用できるかあてにならなそうだったのもあり、特に水平方向のスケーリングには不安のある状態でした。
Goghでは新たにクラスタリングをするよりはサーバーレスなサービスを使用したほうが運用負荷も低くスケールさせやすいだろうという判断で採用しています。
また元々xambrでは上記の通りRedisを使用していたのもあり、Go用のRedis互換SDKを用意していただいたのもあり、既存コードの多くをそのまま使用できたのも採用理由になりました。

@[card](https://github.com/momentohq/momento-go-redis-client)

### IaC:既存資産の活用

Infrastructure as Code（以下IaC）に関してはxambrとほとんど戦略を変えていません。
AWS CDK Goを使用して記述し、GitHub Actionsで実行を自動化してデプロイを簡略化しています。

@[card](https://docs.aws.amazon.com/ja_jp/cdk/v2/guide/work-with-cdk-go.html)

@[card](https://github.co.jp/features/actions)

このあたりの話に関してはイベントでの登壇でも詳しくお話をさせていただきました。

@[card](https://zenn.dev/ambr_inc/articles/87352ca4ba7436)

@[docswell](https://www.docswell.com/s/qazx7412/Z3G77V-2024-12-03-202945)

そこでも述べたのですが、xambrでCDKの言語にGoを採用したのは結果的にあまり良い選択ではなかったと考えています。
ただそれでも今回はGoによる記述を継続することを選択しました。
これは初期の開発をある程度急いで、クライアント側に対して実環境を早めに提供して開発できる体制を確立するべきだと考えていたからです。

この項最後にここまで出す機会がなかったインフラ構成図を載せておきます。（内容は上記登壇スライドの再掲です）
特徴としては採用サービスをサーバーレス/マネージドサービスに振り切った結果VPCレスで運用できているところでしょうか。
また章を切るほどの内容ではないのですがGoghではステッカー機能などに使われている画像アップローダー機能はS3の署名付きURLを使用する形でこれもサーバーレスに寄せた構成にしています。
（SQSとかは現状入れていないですがそれ以外は教科書的な構成だと思う）

![](/images/3ea67a78d7c5c4/aws.png)

## コードのアーキテクチャ戦略:モジュール的分割の理想

コード側のアーキテクチャ戦略に関しては大きな変更がありました。
それはDDDを辞める、もっと正確に言うなら戦術的DDDを辞めるという選択をしました。

### 全体の戦略:DDDやめました（戦術）

まず大前提として弊社ではxambrから一貫してAPIサーバーはGo/Ginという構成を取り続けてきました。
これはよくGoで言われるパフォーマンスという観点よりは、可搬性の高いシングルバイナリになることでインフラ構成やIaCのコードの複雑性を減らすことができることを主眼としたものでした。
ただGoにはRailsのようなフルスタックなフレームワークと違って自分たちでアーキテクチャ戦略を考えなければなりません。
そこでDDDのすべてを放棄するのではなく、戦術的DDDに出てくる各要素をうまくピックアップしつつその（戦術的DDDの）教義に沿うことを目的にはしないという選択をしました。
具体的には例えばusecaseやrepositoryのような戦術的DDDで出てくる要素は使いつつ、DI等は使用せず依存性の逆転をしていなかったりします。
この場合GoだとInterfaceやDIライブラリ（[wire](https://github.com/google/wire)とか）でDIを律儀に行おうとするとコードが煩雑になり運用も複雑になるというところがあり、テスト時には[Testcontainers](https://testcontainers.com)等を使用してモックやスタブを使用しない開発体制であればメリットも享受しにくいのでデメリットを受け入れてまで採用はしないということです。

またDDD以外の具体的な実装に関しては、まず（Git）リポジトリ的にはモジュラーモノリス的に1つのリポジトリに集約しつつディレクトリはアプリケーションとドメイン関心事ごとで切っています。
ただしこれもモジュラーモノリスの定義自体に厳密なのではなくあくまでざっくりディレクトリを切り認知負荷を軽減させることが目的であり、完璧なモジュールとして依存性を排除することは目指しません。

ちなみにこの話はもう少し一般化した話を過去に登壇、スライド化しているので詳しく知りたい方はそちらを読んでいただければと思います。

@[docswell](https://www.docswell.com/s/qazx7412/ZQRVLY-2025-06-13-100842)

### 管理画面:頑張らないフロントエンド

執筆時現在、Goghにすこしだけ存在しているWebフロントエンドに関しても触れておきましょう。
Goghのバックエンドには社内向け管理画面としていくらかの画面が存在しています。

元々xambr時代には、Next.jsで作られた管理画面が存在していました。
ただこれはあくまでUnityアプリケーション向けのWebAPIをメインとするambrのサーバーサイドチームにとって、技術スタックの違いやNext.js自体のやりたいことに対するオーバースペック感と実装、運用コストの重さから大きな課題として認識されていました。
xambrはその後、Slack botを介して管理画面に相当する機能を提供するという荒業に舵を切ったのですが、Goghとしてはサービスの性質上将来的にSlackの実質CLIベースのUIでは実現できない機能を求められる可能性を考えて画面を用意することにしました。

ただ現代では「ちゃんとしたフロントエンド」というものはちょっとした管理画面程度の開発にとってはかなりオーバースペックで、むしろサーバーサイドチームによって運用される前提に立つとあまりにフロントエンドとしての専門のスキルを要求されすぎるきらいがあります。
なので苦肉の策ですが、今回Goghでは伝統的なWebのフロントエンドたるテンプレートエンジンに立ち返ることにしました。
選定してる側としても正直この選択がベストなのかと言われればそうではないとは思ってはいますが、現実的に取れる選択肢としてはベターというのが私の結論ではあります。^[まぁその上で今から新しく作るならHono JSXとかも選択肢にはなったのかも…とはいえ後知恵]
一応結果的にはGoのテンプレートエンジンを使ったSSRをLambdaでホストする形にしたことで、管理画面のサーバーをLambda単体で完結できたという点も利点ではありました。

## 運用してみて:それぞれに見えてきたこと

ここまで各技術の選定理由に触れて来たわけですがもちろんじゃあ当然実際に運用してみてどうだったんだという話になりますね。
実際ありがたいことに多くのユーザーの皆様にアクセスをいただくことになりました。

@[tweet](https://x.com/goghJPN/status/1988412142199206061)

あまりここらへんの話は詳細にはお話しませんが、基本的には（Webバックエンドとしては）アクセス数は緩やかな右肩上がりでの増加をしました。
なので此処から先の話はその前提で読んでいただければと思います。

### App Runner:想定外の過剰スペック？

App Runnerに関して一番難しかったのはスケーリング周りかなと自分では思っています。
まずApp Runnerのオートスケールは、「Concurrency^[正確にはその最大数を管理するMax Concurrency]」という単位で管理します。
これは1インスタンスあたりの同時リクエスト数を意味するもので、この値を超えると新しいインスタンスが起動します。
つまりCPUやメモリなどのスペックの足りなさをトリガーにしてスケーリングさせることができず、1インスタンスにどれだけの同時実行を許容させるか、そもそも1実行がどれほどの負荷になるのか、そもそも自分たちのサービスではこの「Concurrency」というあまり馴染みのない単位はどういう挙動をするのかすべて調査、測定しつつ自分で設定する必要があります。
ambrとしてはxambr時代からApp Runnerを使ってはいますが、正直この「Concurrency」というものに対して高い解像度で臨めているとはあまり言い難い状況ではあり、実際の負荷状況を見ながらなんとなくの数値で決めざるを得ないのが正直なところです。

また上記の話とは別にあまりスケーリングしないということがあります。
こう書くとなにか技術的な失敗や問題が発生しているのかと思われてしまうかもしれないのですがそうではなく、本当にあまりスケーリングしない、もっと正確に言うならスケーリングを必要としない状況になっています。
前提の通り、アクセス数の急激な上昇をそこまでしていないというのはあるにせよ、根本としてアクセス数ベースで考えると「思ったよりインスタンス数が少ない」です。
具体的な数字はお教えできませんが、Goghはかなり少ないインスタンスで運営できています。
なんならあまりに余裕があるのでインスタンスのスペックや、Concurrencyの設定は初期から下げているくらいです。
考えられる原因としてはいくつかあるにはあります。
1つにこのサービスでは一部の処理をS3に寄せることで運用負荷の軽減とスケーリングの実現をしているところがあります。
一部ユーザーが作成した情報は、取得時はS3（とCloudFront）より配信されるので通常のWebサービスよりは負荷が少なくなるようにできてはいます。
ただこれもあくまで一部でありここまで極端なスペックの余裕の説明としては腑に落ちないところではあります。
またもう1つに言語選定にGoを使用しているというのもありえると考えています。
コードのアーキテクチャ戦略の章で触れたとおり、GoghでGoを選択したのはあくまでIaCの複雑さの軽減というところが大きかったです。
ただGo/Ginという構成は確かにRuby/RailsやTS/honoのようないわゆる^[マサカリ避け]スクリプト言語や、JVMや.NET系のいわゆる^[やはりマサカリ避け]VM言語に比べて、ネイティブで動作するのでそれらと比較してパフォーマンス的にかなり有利であるとは思われます。
自分としては完全に腑に落ちているのではないですが、まあS3に原因を求めるよりはそれらしいのかなと考えているところではあります。

### PlanetScale:爆速マイグレーションとGC

PlanetScaleに関しては新たに導入したサービスの中では一番の「あたり」だったなと思います。
まずブランチ機能に関しては本当に開発体験が良いです。
もちろん作業ブランチから安全に作業できるというのも大きいですが、もう1つ運用して良かったこととして、マイグレーションが爆速で終わります。
比喩ではなく本当に爆速です。
それなりにレコード数のあるテーブルへのマイグレーションがものの数秒〜数分で終了します。

![](/images/3ea67a78d7c5c4/migration.png)

PlanetScaleはVitessベースで複数ノードによる分散処理を行っているため、オンラインでのスキーマ変更が高速に実行できるのだと思われます。とにかく早くて楽です。
これだけでも使用する理由になりますが、その上でAurora比で低いスペックのインスタンスが選択できるのでかなり低価格で運用ができています。
ただ懸念になっている点もあり、こちらの使用方法にも問題があるとは思うのですが、メモリに関しては少し微妙な挙動をしています。メトリクスを見ているとメモリ（バッファプール等）の解放があまり頻繁には行われていないようで、高い利用率で推移した後に一気にメモリが解放されたと見受けられる挙動をしています。スケールアップ以外に解決する手段がないか模索しているところではあります。

### Momento:レートリミットという天蓋

Momentoに関しては良いところもあれば良くないところもあったなというのが正直なところです。
実際クラスタリングやインスタンスのスケールのことを考えなくても良くなったことは非常に大きく、Redisに関する知見の少ないチームとしてはその点はすごく良かったところではあります。
ただ色々なデータをMomentoに放り込んだ結果として、特にキャッシュを多用する機能を担当してもらったメンバーからはレートリミットに関して心配する声が上がりました。
現状ではまだ問題はないのですが、サーバーレスのスケーリングとして壁になるものとしてレートリミットは大きいです。
なにか機能を実装をするときにレートリミットを意識すると、本来キャッシュを使用すべき処理もキャッシュの使用を避けようといった話になってしまいまた変な負荷が回避策によって発生し負のピタゴラスイッチが発生しがちです。
とはいえレートリミットというものもベンダーとのコミュニケーションによって解放してもらうにも限界があることを考えるといずれ契約プランの変更やそもそもの全体アーキテクチャの見直しという話になってくるので意思決定の難しさを痛感しているところではあります。

### アーキテクチャ:モジュール的分割の現実

コード側のアーキテクチャに関して、DDDを辞める（と言いつつ微妙に辞めない）ことやモジュール的に分割することによってxambr時代よりは幾分かは見通しがよくなったように感じてはいます。
ただ実際に運用してみて今大きく課題だなと感じるところとして、モジュール的に分割した各ディレクトリに関して、実装を担当するメンバーに裁量を任せた結果それぞれが大きく書き方が変わってしまったところがあります。
実装メンバーに裁量を任せたのは、開発初期は開発スピードを上げたかったのと、自分があまりマイクロマネジメントをしすぎないほうがいいかなというところによるものでした。
なので状況的には仕方がないことではあるのですが、これは大きな技術的負債になってしまった感じがあります。
また今回Repository層に関しては分割したディレクトリの外に置いてしまったのですが、これは肥大化して見通しが悪くなっているところがあるのでこれも技術的負債というところではあります。
ただこういったアーキテクチャ論は正解がなく、技術的負債を発生させないというのも難しいものですし、結果的にこれで運用できているというのもあります。
また前者に関しては、自分がリードエンジニアとして開発速度を犠牲にしても強権的に書き方を揃えることを強く進めるべきだったというのは未だになんとも言えないなと考えることもあります。
まあ何にせよ結局アレコレ工夫をしながらやっていくしかないのかなと思いますね。

## おわりに

というわけでここまでモバイル版Goghのバックエンドに関してリードエンジニアである自分の立場での振り返りをしてきました。
駆け足での紹介となり、特に後半失速気味でしたが正直これを書いているタイミングで締め切りギリギリなのでご容赦いただけると幸いでございます。
これからもモバイル版Goghに関しては今後のアクセス増加をユーザーに影響がないように捌きつつ機能開発もしていければと思っておりますので、ユーザーの皆様におかれましては何卒生暖かい目で見守って頂ければと思います。来年も引き続きのご愛顧をよろしくお願い申し上げます。
